We implemented three different models: the first is a Stacking Classifier with Logistic Regression, SVM, Random Forest, and XGBoost, to which we applied a Grid Search only on the main hyperparameters for lightweight tuning using a 5-fold Cross-Validation (\texttt{cv = StratifiedKFold(n\_splits=5)}). Using Logistic Regression as a meta-model allowed us to combine the predictions of the base models with low complexity.

For both stacking models, we standardized the features for the base learners that are sensitive to feature scaling: SVM and Logistic Regression in the first stacking model, and KNN in the second one.

In the third model, as in the previous two, we use a 5-fold stratified CV and Grid Search to optimize the main parameters. For this model, we used a Logistic Regression in a pipeline that sequentially performs feature standardization with \texttt{StandardScaler} and trains the Logistic Regression model. This model is lightweight, has low variance in CV results, and is stable and easy to understand how features influence prediction. However, it does not capture complex interactions well and is less efficient than stacking models.

For the two stacking models, we trained all combinations of the base learners used in the stacking model and chose the best one (based on the mean CV accuracy). For all three models, we also looked at permutation importance for the features and tried deleting the least important ones but, in the end we always saw a drop in the score, so we decided to maintain the original features (except for total\_sp\_defense\_difference, as mentioned before).
