The goal of this work is to create a model capable of predicting the winner of a Pokémon battle using only the available information from the first 30 turns.

The first step was to load the .jsonl dataset, cleaning the set by eliminating battle 4877 due to its incorrect nature. By creating a vocabulary, we assigned the respective base stats to each Pokémon by exploring \texttt{p1\_team\_detail}, \texttt{p2\_lead\_details}, and the Pokémon present in the \texttt{battle\_timeline} (using the \texttt{build\_species\_index}).

We derived the players' lead speed using \texttt{get\_lead\_speed}, used the \texttt{track\_pokemon\_conditions} function to summarize the first 30 turns, taking into account the final HP, voluntary switches (excluding KOs) and last-turn effects weighted by 0.5, since we decided to treat effects as having half the impact of status conditions. We also computed the difference between P1 and P2 for the six base stats using the function \texttt{compute\_differences\_base\_stats}.With \texttt{extract\_boost\_difference}, we consider only the stat boosts applied to the Pokémon in the two groups and compute their difference.Using \texttt{extract\_accuracy\_difference}, we evaluate whether each player tended toward risky or safe moves by computing their average move accuracy and returning the difference between the two.

Using the \texttt{create\_feature} function, we merge all the features into a DataFrame, focusing on the first few rows to verify the adequacy of the extracted features. We then created a correlation matrix, noticing identical values, and found a perfect correlation between total\_sp\_attack\_difference and total\_sp\_defense\_difference. To avoid redundancies, we removed total\_sp\_defense\_difference.

We implemented three different models: the first is a Stacking Classifier with Logistic Regression, SVM, Random Forest, and XGBoost, to which we applied a Grid Search only on the main hyperparameters for lightweight tuning using a 5-fold Cross-Validation (\texttt{cv = StratifiedKFold(n\_splits=5)}). Using Logistic Regression as a meta-model allowed us to combine the predictions of the base models with low complexity.

For both stacking models, we standardized the features for the base learners that are sensitive to feature scaling: SVM and Logistic Regression in the first stacking model, and KNN in the second one.

In the third model, as in the previous two, we use a 5-fold stratified CV and Grid Search to optimize the main parameters. For this model, we used a Logistic Regression in a pipeline that sequentially performs feature standardization with \texttt{StandardScaler} and trains the Logistic Regression model. This model is lightweight, has low variance in CV results, and is stable and easy to understand how features influence prediction. However, it does not capture complex interactions well and is less efficient than stacking models.

Unfortunately, since we don't have access to the private leaderboard, we can evaluate our models using the public leaderboard,combined with local cross-validation accuracy, where stacking models appear to learn from the data better than simple Logistic Regression. For the two stacking models, we trained all combinations of the base learners used in the stacking model and chose the best one (based on the mean CV accuracy). For all three models we also looked at permutation importance for the features and tried deleting the least important ones but, in the end we always saw a drop in the score, so we decided to maintain the original features (except for total\_sp\_defense\_difference, as mentioned before). We included Logistic Regression to see if a linear model could compete with more complex stacking models. In summary, stacking models offer better performance at the cost of more careful management.

