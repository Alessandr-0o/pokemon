The goal of this work is to create a model capable of predicting the winner of a Pokémon battle using only the available information from the first 30 turns.

The first step was to load the .jsonl dataset, cleaning the set by eliminating battle 4877 due to its incorrect nature. By creating a vocabulary, we assigned the respective base stats to each Pokémon by exploring \texttt{p1\_team\_detail}, \texttt{p2\_lead\_details}, and the Pokémon present in the \texttt{battle\_timeline} (using the \texttt{build\_species\_index}).

We derived the players' lead speed using \texttt{get\_lead\_speed}, used the \texttt{track\_pokemon\_conditions} function to summarize the first 30 turns, taking into account the final HP, voluntary switches (excluding KOs) and last-turn effects weighted by 0.5, since we decided to treat effects as having half the impact of status conditions. We also computed the difference between P1 and P2 for the six base stats using the function \texttt{compute\_differences\_base\_stats}.With \texttt{extract\_boost\_difference}, we consider only the stat boosts applied to the Pokémon in the two groups and compute their difference.Using \texttt{extract\_accuracy\_difference}, we see whether a player used risky or safe moves.

Using the \texttt{create\_feature} function, we merge all the features into a DataFrame, focusing on the first few rows to verify the adequacy of the extracted features. We then created a correlation matrix, noticing identical values, and found a perfect correlation between total\_sp\_attack\_difference and total\_sp\_defense\_difference. To avoid redundancies, we removed total\_sp\_defense\_difference.

We implemented three different models: the first is a Stacking Classifier with Logistic Regression, SVM, Random Forest, and XGBoost, to which we applied a Grid Search only on the main hyperparameters for lightweight tuning using a 5-fold Cross-Validation (\texttt{cv = StratifiedKFold(n\_splits=5)}). Using Logistic Regression as a meta-model allowed us to combine the predictions of the base models with low complexity. The good (not optimal) effectiveness of this stacking on the dataset is highlighted by the stable accuracy of the CV, but it turns out to be inflexible. It works well because the model is stable, but it poorly captures the interactions between the base models.

The second model is a stacking with an XGBoost meta-model which, in addition to the base models already mentioned in the previous model, also includes the KNN. Since XGBoost is non-linear, the accuracy improves compared to the previous stacking, but the complexity and risk of overfitting increase.

In the third model, as in the previous two, we use a 5-fold stratified CV and Grid Search to optimize the main parameters. For this model, we used a Logistic Regression in a pipeline that sequentially performs feature standardization with \texttt{StandardScaler} and trains the Logistic Regression model. This model is lightweight, has low variance in CV results, and is stable and easy to understand how features influence prediction. There are no cases of overfitting. However, it does not capture complex interactions well and is less efficient than stacking models.

Unfortunately, since we don't know the private leaderboard tests, we can evaluate our models using the public leaderboard, where stacking models appear to ``learn'' from the data better than Logistic Regression. For the stacking models (with the help of ChatGPT), we used a function that trained all variants of the base model, choosing the two with the best local accuracies. We included Logistic Regression to see if a linear model could compete with more complex stacking models. In summary, stacking models offer better performance at the cost of more careful management.

